# Modernized Architecture Overview

## Current Microservices Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          Client Applications                         ‚îÇ
‚îÇ                     (Web, Mobile, Third-party)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚îÇ HTTP/HTTPS
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Broker Service (API Gateway)                  ‚îÇ
‚îÇ                           Port: 8080                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Request routing & orchestration                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ CORS handling                                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Logging & recovery middleware                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Request validation                                         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ             ‚îÇ              ‚îÇ             ‚îÇ
    ‚îÇ             ‚îÇ              ‚îÇ             ‚îÇ
    ‚ñº             ‚ñº              ‚ñº             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Auth   ‚îÇ  ‚îÇ Logger  ‚îÇ  ‚îÇ Listener ‚îÇ  ‚îÇ   Future    ‚îÇ
‚îÇ Service ‚îÇ  ‚îÇ Service ‚îÇ  ‚îÇ Service  ‚îÇ  ‚îÇ  Services   ‚îÇ
‚îÇ:8081    ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ            ‚îÇ            ‚îÇ
     ‚îÇ            ‚îÇ            ‚îÇ
     ‚ñº            ‚ñº            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Data Layer                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇPostgres ‚îÇ  ‚îÇ MongoDB ‚îÇ  ‚îÇ  Redis  ‚îÇ  ‚îÇRabbitMQ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  :5432  ‚îÇ  ‚îÇ :27017  ‚îÇ  ‚îÇ  :6379  ‚îÇ  ‚îÇ  :5672  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ         ‚îÇ  ‚îÇ         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Users  ‚îÇ  ‚îÇ  Logs   ‚îÇ  ‚îÇ Caching ‚îÇ  ‚îÇ  Events ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   DB    ‚îÇ  ‚îÇ   DB    ‚îÇ  ‚îÇ  & Geo  ‚îÇ  ‚îÇ  Queue  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Service Details

### 1. Broker Service (API Gateway)

**Port:** 8080  
**Technology:** Go, Chi Router, gRPC, RabbitMQ  
**Purpose:** Central entry point for all client requests

**Features:**

-   Request routing to appropriate microservices
-   CORS configuration
-   Common middleware (logging, recovery)
-   Request validation using validator/v10
-   Integration with RabbitMQ for async messaging
-   gRPC client for logger service

**Endpoints:**

-   `POST /` - Health check
-   `POST /handle` - Main request handler (routes based on action)
-   `POST /log-grpc` - gRPC logging

### 2. Authentication Service

**Port:** 8081  
**Technology:** Go, Chi Router, JWT, PostgreSQL  
**Purpose:** User authentication and JWT token management

**Features:**

-   JWT-based authentication (access + refresh tokens)
-   Password hashing with bcrypt
-   Token validation for other services
-   Environment-based JWT configuration
-   Input validation

**Endpoints:**

-   `POST /authenticate` - Login and get JWT tokens
-   `POST /refresh` - Refresh access token
-   `POST /validate` - Validate JWT token
-   `GET /ping` - Health check

**Database:** PostgreSQL (users table)

### 3. Logger Service

**Port:** Internal  
**Technology:** Go, Chi Router, gRPC, MongoDB  
**Purpose:** Centralized logging for all services

**Features:**

-   HTTP REST API for logging
-   gRPC server for high-performance logging
-   RPC interface for legacy support
-   MongoDB for log persistence
-   Structured log storage

**Endpoints:**

-   `POST /log` - Write log entry
-   gRPC service for remote logging

**Database:** MongoDB (logs collection)

### 4. Listener Service

**Technology:** Go, RabbitMQ  
**Purpose:** Event-driven message consumer

**Features:**

-   Consumes messages from RabbitMQ
-   Event-driven architecture support
-   Async processing
-   Integration with logger service

## Common Library Package

```
common/
‚îú‚îÄ‚îÄ request/
‚îÇ   ‚îî‚îÄ‚îÄ request.go      # Request parsing & validation
‚îú‚îÄ‚îÄ response/
‚îÇ   ‚îî‚îÄ‚îÄ response.go     # Standardized API responses
‚îú‚îÄ‚îÄ middleware/
‚îÇ   ‚îî‚îÄ‚îÄ middleware.go   # Logger & Recovery middleware
‚îî‚îÄ‚îÄ jwt/
    ‚îî‚îÄ‚îÄ jwt.go          # JWT generation & validation
```

**Shared Across All Services:**

-   Standardized request/response handling
-   Input validation using go-playground/validator
-   JWT token utilities
-   Common middleware (logging, panic recovery)
-   Reduces code duplication

## Data Stores

### PostgreSQL

-   **Purpose:** Relational data (users, rides, drivers)
-   **Version:** 16-alpine
-   **Features:** ACID compliance, complex queries
-   **Future Use:** User accounts, ride data, driver profiles

### MongoDB

-   **Purpose:** Document storage (logs, events)
-   **Version:** 7-jammy
-   **Features:** Flexible schema, fast writes
-   **Current Use:** Application logs

### Redis

-   **Purpose:** Caching & real-time data
-   **Version:** 7-alpine
-   **Features:** In-memory, geo-spatial queries
-   **Future Use:**
    -   Real-time driver location tracking
    -   Session management
    -   Rate limiting
    -   Cache frequently accessed data

### RabbitMQ

-   **Purpose:** Message queue for async communication
-   **Version:** 3.13-management-alpine
-   **Features:** Reliable messaging, pub/sub
-   **Current Use:** Event-driven communication between services

## Communication Patterns

### 1. Synchronous HTTP/REST

```
Client ‚Üí Broker Service ‚Üí Authentication Service
         ‚Üì
         Response with JWT tokens
```

### 2. Asynchronous Messaging (RabbitMQ)

```
Service A ‚Üí RabbitMQ ‚Üí Listener Service ‚Üí Process Event
```

### 3. gRPC (High Performance)

```
Broker Service ‚Üí gRPC ‚Üí Logger Service (fast logging)
```

### 4. RPC (Remote Procedure Call)

```
Service ‚Üí RPC Client ‚Üí Logger Service (legacy support)
```

## Security Features

### Authentication

-   ‚úÖ JWT-based authentication
-   ‚úÖ Access tokens (24h expiry)
-   ‚úÖ Refresh tokens (168h expiry)
-   ‚úÖ Password hashing (bcrypt)
-   ‚úÖ Token validation endpoint for other services

### Middleware

-   ‚úÖ CORS configuration
-   ‚úÖ Request size limits (1MB)
-   ‚úÖ Panic recovery
-   ‚úÖ Structured logging

### Input Validation

-   ‚úÖ Validator/v10 for struct validation
-   ‚úÖ Email format validation
-   ‚úÖ Password length requirements
-   ‚úÖ Detailed validation error responses

## Health Checks & Monitoring

### Docker Health Checks

```yaml
PostgreSQL: pg_isready (10s interval)
MongoDB: mongosh ping (10s interval)
Redis: redis-cli ping (10s interval)
RabbitMQ: rabbitmq-diagnostics (30s interval)
Services: HTTP endpoint checks (30s interval)
```

## Communication Pattern Analysis

### Why HTTP + gRPC Hybrid Architecture?

This system uses **both HTTP and gRPC** intentionally. Here's the rationale:

#### HTTP/REST (External Communication)

**Used by:** API Gateway external endpoints (`/handle`, `/grpc/auth`, `/grpc/log`)

**Reasons:**

-   ‚úÖ **Browser compatibility** - JavaScript fetch/XMLHttpRequest works out-of-the-box
-   ‚úÖ **Debugging** - curl, Postman, browser DevTools can inspect requests
-   ‚úÖ **Human-readable** - JSON payloads easy to read/debug
-   ‚úÖ **Widely understood** - Industry standard, extensive documentation
-   ‚úÖ **Firewall-friendly** - Works through proxies/firewalls without special config

**Trade-offs:**

-   ‚ùå Slower serialization (JSON vs protobuf: ~5-10x size difference)
-   ‚ùå No streaming support without WebSocket
-   ‚ùå Schema enforcement requires manual validation

#### gRPC (Internal Service-to-Service)

**Used by:** API Gateway ‚Üí Auth Service (port 50051), API Gateway ‚Üí Logger Service (port 50052)

**Reasons:**

-   ‚úÖ **Performance** - Protobuf serialization is 5-10x faster than JSON
-   ‚úÖ **Type safety** - Generated code from .proto files prevents type mismatches
-   ‚úÖ **Streaming** - Bidirectional streaming built-in (future real-time features)
-   ‚úÖ **HTTP/2** - Multiplexing, server push, header compression
-   ‚úÖ **Contract-first** - .proto files serve as API documentation

**Trade-offs:**

-   ‚ùå Not browser-compatible (needs gRPC-web proxy)
-   ‚ùå Harder to debug (binary protocol, need special tools)
-   ‚ùå Requires protobuf compilation step

**Performance Comparison (Auth Request):**
| Protocol | Payload Size | Latency | Notes |
|----------|--------------|---------|-------|
| HTTP/REST | ~450 bytes | 400ms | JSON overhead, but negligible vs bcrypt (200ms) |
| gRPC | ~180 bytes | 395ms | 5ms saved, but bcrypt dominates |

**Verdict:** For auth operations where bcrypt takes 200ms, gRPC's 5ms advantage is marginal. We use gRPC for **future scalability** (when adding real-time features like driver location streaming).

---

### Port Exposure Analysis

#### Current Configuration (Development)

```yaml
api-gateway: 8080 (HTTP) ‚úÖ PUBLIC
authentication: 8081 (HTTP) + 50051 (gRPC) ‚ö†Ô∏è BOTH PUBLIC
logger: 8082 (HTTP) + 50052 (gRPC) ‚ö†Ô∏è BOTH PUBLIC
```

#### Why Both Ports Exposed in Development?

1. **HTTP Port (8081, 8082):**

    - `/metrics` endpoint for Prometheus scraping
    - `/authenticate`, `/log` endpoints for direct testing/debugging
    - Health checks (`/ping`)

2. **gRPC Port (50051, 50052):**
    - API Gateway calls these internally
    - Direct gRPC testing with tools like `grpcurl`

#### Production Architecture (Recommended)

```yaml
api-gateway: 8080 (HTTP) ‚úÖ PUBLIC ONLY
authentication: 80 (HTTP) + 50051 (gRPC) üîí INTERNAL DOCKER NETWORK
logger: 80 (HTTP) + 50052 (gRPC) üîí INTERNAL DOCKER NETWORK
```

**Changes:**

-   Remove port mappings for auth/logger in docker-compose.yml
-   API Gateway reaches auth via `authentication-service:50051` (Docker DNS)
-   Prometheus scrapes `authentication-service:80/metrics` (Docker DNS)
-   External clients **cannot** directly call auth/logger services

**Why This Is Correct:**

-   ‚úÖ **Least Privilege Principle** - Services only expose what's necessary
-   ‚úÖ **Attack Surface Reduction** - Cannot brute-force auth service directly
-   ‚úÖ **Enforced Gateway Pattern** - All traffic goes through API Gateway (rate limiting, auth checks)
-   ‚úÖ **Internal Prometheus Scraping** - No need for public metrics endpoints

**See [DEPLOYMENT.md](./DEPLOYMENT.md) for production configuration.**

---

### Metrics Collection Architecture

#### Problem: How Does Prometheus Scrape Internal Services?

**Misconception:** "If auth/logger services are internal-only, how does Prometheus get metrics?"

**Answer:** Prometheus runs **inside the Docker network**.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Docker Bridge Network                 ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Prometheus   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ authentication:80   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ     ‚îÇ /metrics            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ             Internal DNS resolution  ‚îÇ
‚îÇ         ‚îÇ                                       ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ                      ‚îÇ logger:80           ‚îÇ  ‚îÇ
‚îÇ                      ‚îÇ /metrics            ‚îÇ  ‚îÇ
‚îÇ                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ñ≤ Port 9090 exposed for UI access
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Browser   ‚îÇ http://localhost:9090
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Prometheus Configuration:**

```yaml
# project/prometheus.yml
scrape_configs:
    - job_name: "authentication-service"
      static_configs:
          - targets: ["authentication-service:80"] # Docker DNS, not localhost:8081
```

**Key Point:** Prometheus does NOT use `localhost:8081`. It uses **internal Docker DNS**: `authentication-service:80`.

**Production Equivalent:**

-   In Kubernetes: Prometheus uses service discovery to find pods
-   In AWS ECS: Prometheus scrapes via private IP addresses
-   No need for public port exposure

---

### Why Not Just HTTP OR Just gRPC?

#### Option A: HTTP Only (No gRPC)

**Rejected because:**

-   ‚ùå Lose type safety (manual JSON validation everywhere)
-   ‚ùå No streaming support (need polling for real-time features)
-   ‚ùå Slower serialization (JSON parsing overhead)
-   ‚ùå Cannot leverage HTTP/2 multiplexing efficiently

**Good for:** Public APIs, third-party integrations, browser clients

#### Option B: gRPC Only (No HTTP)

**Rejected because:**

-   ‚ùå Not browser-compatible (would need gRPC-web proxy)
-   ‚ùå Harder debugging (binary protocol, need `grpcurl` or Postman's gRPC feature)
-   ‚ùå Curl/Postman can't directly test (protobuf encoding required)
-   ‚ùå Learning curve for frontend developers

**Good for:** High-performance internal services, mobile apps (gRPC-native)

#### Option C: Hybrid (Current Choice) ‚úÖ

**Benefits:**

-   ‚úÖ HTTP for external API Gateway endpoints (developer-friendly)
-   ‚úÖ gRPC for internal service calls (performance + type safety)
-   ‚úÖ Best of both worlds for different use cases
-   ‚úÖ Future-proof for real-time features (gRPC streaming)

**Trade-off:** Slightly more complex (maintain both HTTP handlers and gRPC services)

---

### Authentication Latency: Is 400ms a Problem?

**Observed Performance:**
| Endpoint | P50 | P95 | P99 |
|----------|-----|-----|-----|
| `/handle` (auth) | 380ms | 420ms | 450ms |
| `/handle` (log) | 2ms | 5ms | 8ms |

**Why Auth is Slow:**

```
Total: ~400ms
‚îú‚îÄ‚îÄ bcrypt password hashing: 200ms (50%)
‚îú‚îÄ‚îÄ PostgreSQL query: 100ms (25%)
‚îú‚îÄ‚îÄ JWT generation (RSA): 50ms (12.5%)
‚îú‚îÄ‚îÄ gRPC communication: 30ms (7.5%)
‚îî‚îÄ‚îÄ HTTP parsing/validation: 20ms (5%)
```

**Is This Acceptable? YES!**

1. **Bcrypt is intentionally slow:**

    - Cost factor of 12 = ~200ms (industry standard)
    - Prevents brute-force attacks (attacker needs 200ms per guess)
    - AWS Cognito, Auth0, Firebase Auth all have similar latencies

2. **Login is infrequent:**

    - Users login once per day/week
    - Subsequent requests use cached JWT (validated in ~5ms)
    - Not a bottleneck for system throughput

3. **Faster = Less Secure:**
    - Reducing bcrypt cost to 8 ‚Üí 50ms latency BUT 16x easier to brute-force
    - Security > Speed for authentication

**Optimization Options (if needed):**

-   Cache bcrypt hashes in Redis (risky, reduces security)
-   Use Argon2id instead of bcrypt (slightly faster, more memory-hard)
-   Offload JWT generation to hardware security module (HSM)

---

### Logging Latency: Why So Fast (2ms)?

**Fire-and-Forget Architecture:**

```
API Gateway ‚Üí gRPC call ‚Üí Logger Service ‚Üí Return immediately
                                         ‚Üì
                              (Async) Write to MongoDB
```

**Key Design Decision:**

-   Logger service returns success **before** writing to MongoDB
-   Uses RabbitMQ for guaranteed delivery (survives crashes)
-   Logging never blocks critical path (user response)

**Trade-off:**

-   ‚úÖ Ultra-fast response time (2ms)
-   ‚ùå Logs might be delayed by a few seconds
-   ‚ùå If logger crashes before flushing, logs might be lost

**Acceptable Because:**

-   Logs are for debugging, not critical business logic
-   RabbitMQ ensures messages aren't lost (persistent queue)
-   MongoDB write is async (won't block user experience)

---

## Technology Choices & Justifications

### Why PostgreSQL for Users?

-   ‚úÖ ACID compliance (critical for financial transactions)
-   ‚úÖ Foreign keys (enforce referential integrity)
-   ‚úÖ Complex queries (JOIN operations for analytics)
-   ‚ùå Slower writes vs NoSQL (acceptable trade-off for consistency)

### Why MongoDB for Logs?

-   ‚úÖ Schema-less (logs have variable fields)
-   ‚úÖ Fast writes (append-only, no transactions needed)
-   ‚úÖ TTL indexes (auto-delete old logs after 30 days)
-   ‚ùå No ACID guarantees (acceptable for logs)

### Why Redis (Planned)? (Depend on you mostly tho)

-   ‚úÖ Geo-spatial queries (GEORADIUS for driver matching)
-   ‚úÖ In-memory speed (<1ms latency)
-   ‚úÖ Pub/sub for real-time updates
-   ‚ùå Data loss on crash (use persistence + replication)

### Why RabbitMQ (Not Kafka)?

-   ‚úÖ Simpler setup (single container)
-   ‚úÖ Built-in retries + dead-letter queues
-   ‚úÖ Lower latency (<10ms vs Kafka's ~50ms)
-   ‚ùå Lower throughput vs Kafka (acceptable for our scale)

**When to switch to Kafka:**

-   Need 100k+ messages/second
-   Event sourcing with log compaction
-   Multiple consumer groups reading same events