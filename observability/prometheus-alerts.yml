# Prometheus Alert Rules for UIT-Go
# Based on SLOs defined for Module D Observability
#
# SLOs:
# - Availability: 99.9% (error budget: 0.1% = 43.2 min/month)
# - Trip Booking Success Rate: 99.9%
# - Driver Search P95 Latency: < 200ms
# - Authentication P95 Latency: < 100ms
# - API Gateway P95 Latency: < 500ms

groups:
  # ==========================================
  # SLO-based Alerts - Error Budget Burning
  # ==========================================
  - name: slo_error_budget
    interval: 30s
    rules:
      # Multi-window, multi-burn-rate alerts (Google SRE book pattern)
      # Fast burn: 14.4x burn rate over 1h = 2% budget consumed
      - alert: SLO_HighErrorRate_FastBurn
        expr: |
          (
            sum(rate(http_requests_total{service="api-gateway", status=~"5.."}[1h]))
            / sum(rate(http_requests_total{service="api-gateway"}[1h]))
          ) > (14.4 * 0.001)
        for: 2m
        labels:
          severity: critical
          slo: availability
          burn_rate: fast
        annotations:
          summary: "üî• SLO violation - Fast error budget burn on API Gateway"
          description: |
            Error rate is {{ $value | humanizePercentage }}, burning error budget 14.4x faster than allowed.
            At this rate, monthly error budget will be exhausted in ~2 days.
            
            **Immediate Action Required:**
            1. Check recent deployments
            2. Review Tempo traces for failing requests
            3. Check dependent service health
          runbook_url: "https://github.com/OneKeyCoder/UIT-Go-Backend/blob/master/docs/runbooks/high-error-rate.md"

      # Slow burn: 3x burn rate over 6h = 12.5% budget consumed
      - alert: SLO_HighErrorRate_SlowBurn
        expr: |
          (
            sum(rate(http_requests_total{service="api-gateway", status=~"5.."}[6h]))
            / sum(rate(http_requests_total{service="api-gateway"}[6h]))
          ) > (3 * 0.001)
        for: 15m
        labels:
          severity: warning
          slo: availability
          burn_rate: slow
        annotations:
          summary: "‚ö†Ô∏è SLO at risk - Slow error budget burn on API Gateway"
          description: |
            Error rate is {{ $value | humanizePercentage }}, burning error budget 3x faster than allowed.
            At this rate, monthly error budget will be exhausted in ~10 days.
            
            **Action Required:**
            1. Investigate error patterns in logs
            2. Check for degraded dependencies
            3. Review recent configuration changes

      # Error budget nearly exhausted
      - alert: SLO_ErrorBudgetLow
        expr: |
          (
            1 - (
              sum(rate(http_requests_total{service="api-gateway", status=~"5.."}[30d]))
              / sum(rate(http_requests_total{service="api-gateway"}[30d]))
            )
          ) < 0.999 * 0.2
        for: 5m
        labels:
          severity: critical
          slo: availability
        annotations:
          summary: "üö® Error budget nearly exhausted for API Gateway"
          description: |
            Less than 20% of monthly error budget remaining.
            Consider feature freeze until budget recovers.

  # ==========================================
  # Latency SLO Alerts
  # ==========================================
  - name: slo_latency
    interval: 30s
    rules:
      # API Gateway P95 > 500ms
      - alert: SLO_HighLatency_APIGateway
        expr: |
          histogram_quantile(0.95, 
            sum(rate(http_request_duration_seconds_bucket{service="api-gateway"}[5m])) by (le)
          ) > 0.5
        for: 5m
        labels:
          severity: warning
          slo: latency
          service: api-gateway
        annotations:
          summary: "‚è±Ô∏è API Gateway P95 latency exceeds SLO (500ms)"
          description: |
            P95 latency: {{ $value | humanizeDuration }}
            SLO target: 500ms
            
            **Debug Steps:**
            1. Check Tempo for slow traces
            2. Review downstream service latencies
            3. Check for resource contention (CPU/Memory)

      # Driver Search P95 > 200ms
      - alert: SLO_HighLatency_DriverSearch
        expr: |
          histogram_quantile(0.95, 
            sum(rate(http_request_duration_seconds_bucket{service="location-service", path=~".*nearest.*"}[5m])) by (le)
          ) > 0.2
        for: 5m
        labels:
          severity: warning
          slo: latency
          service: location-service
        annotations:
          summary: "‚è±Ô∏è Driver search P95 latency exceeds SLO (200ms)"
          description: |
            P95 latency: {{ $value | humanizeDuration }}
            SLO target: 200ms
            
            **Debug Steps:**
            1. Check Redis performance
            2. Review GEO query patterns
            3. Check for hot spots in location data

      # Authentication P95 > 100ms
      - alert: SLO_HighLatency_Auth
        expr: |
          histogram_quantile(0.95, 
            sum(rate(http_request_duration_seconds_bucket{service="authentication-service"}[5m])) by (le)
          ) > 0.1
        for: 5m
        labels:
          severity: warning
          slo: latency
          service: authentication-service
        annotations:
          summary: "‚è±Ô∏è Authentication P95 latency exceeds SLO (100ms)"
          description: |
            P95 latency: {{ $value | humanizeDuration }}
            SLO target: 100ms
            
            **Debug Steps:**
            1. Check PostgreSQL query performance
            2. Review bcrypt cost factor
            3. Check JWT signing/validation time

  # ==========================================
  # Service Health Alerts
  # ==========================================
  - name: service_health
    interval: 15s
    rules:
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "üî¥ Service {{ $labels.job }} is down"
          description: |
            Service {{ $labels.job }} has been unreachable for more than 1 minute.
            
            **Immediate Action:**
            1. Check container status: `docker ps -a | grep {{ $labels.job }}`
            2. Check container logs: `docker logs uit-go-{{ $labels.job }}-1`
            3. Check resource usage on host

      - alert: HighMemoryUsage
        expr: |
          (
            process_resident_memory_bytes / (1024 * 1024 * 1024)
          ) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "üíæ High memory usage on {{ $labels.job }}"
          description: |
            Memory usage: {{ $value | humanize1024 }}B
            Consider scaling or investigating memory leaks.

      - alert: HighCPUUsage
        expr: |
          rate(process_cpu_seconds_total[5m]) > 0.8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "üî• High CPU usage on {{ $labels.job }}"
          description: "CPU usage is {{ $value | humanizePercentage }} over the last 5 minutes."

  # ==========================================
  # Business Metrics Alerts
  # ==========================================
  - name: business_metrics
    interval: 30s
    rules:
      # Trip booking failure rate
      - alert: HighTripBookingFailureRate
        expr: |
          (
            sum(rate(http_requests_total{service="trip-service", status=~"5..", path="/trip"}[5m]))
            / sum(rate(http_requests_total{service="trip-service", path="/trip"}[5m]))
          ) > 0.01
        for: 5m
        labels:
          severity: critical
          business_impact: high
        annotations:
          summary: "üöó High trip booking failure rate"
          description: |
            Trip booking error rate: {{ $value | humanizePercentage }}
            This directly impacts revenue and user experience.
            
            **Immediate Action:**
            1. Check trip-service logs for errors
            2. Verify PostgreSQL connectivity
            3. Check payment service integration

      # No trips being created (could indicate system-wide issue)
      - alert: NoTripsBeingCreated
        expr: |
          sum(rate(http_requests_total{service="trip-service", path="/trip", method="POST"}[15m])) == 0
        for: 15m
        labels:
          severity: warning
          business_impact: high
        annotations:
          summary: "üöó No new trips being created"
          description: |
            No trip creation requests in the last 15 minutes.
            This could indicate a frontend issue or system-wide outage.

  # ==========================================
  # Infrastructure Alerts
  # ==========================================
  - name: infrastructure
    interval: 30s
    rules:
      - alert: RabbitMQConnectionLost
        expr: |
          absent(rabbitmq_connections)
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "üê∞ RabbitMQ metrics unavailable"
          description: |
            Cannot collect RabbitMQ metrics. Message queue may be down.
            This affects async logging and event processing.

      - alert: HighInFlightRequests
        expr: |
          http_requests_in_flight{service="api-gateway"} > 100
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "üìà High number of in-flight requests"
          description: |
            {{ $value }} requests currently being processed.
            System may be approaching capacity limits.
